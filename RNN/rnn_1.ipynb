{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a6687c",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Courier New', Courier, monospace; font-size: 30px; font-weight: bold; color: blue; text-align: left;\">\n",
    "Recurrent Neural Networks (RNN) - One-Way Modeling\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69114588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 13:12:43.910689: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-07 13:12:43.932530: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Libraries for data manipulation and visualization\n",
    "import numpy as np                               # For numerical operations\n",
    "import pandas as pd                              # For data manipulation\n",
    "import matplotlib.pyplot as plt                  # For plotting\n",
    "import seaborn as sns                            # For advanced data visualization\n",
    "\n",
    "# Libraries for model building and training\n",
    "import tensorflow as tf                          # For deep learning framework\n",
    "from keras.models import Sequential              # For creating sequential models\n",
    "from keras.layers import Dense, Input, BatchNormalization, Dropout, LeakyReLU, SimpleRNN  # Layers for building neural networks\n",
    "from keras.regularizers import l2               # For L2 regularization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau  # For training optimization\n",
    "\n",
    "# Libraries for evaluation and preprocessing\n",
    "from sklearn.metrics import (                   # For model evaluation metrics\n",
    "    mean_squared_error, \n",
    "    r2_score, \n",
    "    mean_absolute_percentage_error, \n",
    "    median_absolute_error\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler # For standardizing features\n",
    "from sklearn.model_selection import KFold, train_test_split, PredefinedSplit # For cross-validation and splitting data\n",
    "\n",
    "# Set seed for reproducibility\n",
    "GLOBAL_SEED = 50\n",
    "np.random.seed(GLOBAL_SEED)                      # Seed for NumPy\n",
    "tf.random.set_seed(GLOBAL_SEED)                  # Seed for TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a5ffc5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Comprehensive ML - Files & Plots etc/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Time-aware data load (from Data Preparation.ipynb outputs)\u001b[39;00m\n\u001b[32m      3\u001b[39m base_path = \u001b[33m'\u001b[39m\u001b[33m../../Comprehensive ML - Files & Plots etc\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df_train = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/train.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m df_test  = pd.read_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/test.csv\u001b[39m\u001b[33m\"\u001b[39m, parse_dates=[\u001b[33m'\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      7\u001b[39m fold_assignments = np.load(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/train_folds.npy\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/ml001/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/ml001/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/ml001/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/ml001/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/ml001/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../../Comprehensive ML - Files & Plots etc/train.csv'"
     ]
    }
   ],
   "source": [
    "# Time-aware data load (from Data Preparation.ipynb outputs)\n",
    "\n",
    "base_path = '../../Comprehensive ML - Files & Plots etc'\n",
    "\n",
    "df_train = pd.read_csv(f\"{base_path}/train.csv\", parse_dates=['time'])\n",
    "df_test  = pd.read_csv(f\"{base_path}/test.csv\", parse_dates=['time'])\n",
    "fold_assignments = np.load(f\"{base_path}/train_folds.npy\")\n",
    "\n",
    "feature_names = [\n",
    "    'distance', 'frequency', 'c_walls', 'w_walls', 'co2', 'humidity',\n",
    "    'pm25', 'pressure', 'temperature', 'snr'\n",
    "]\n",
    "\n",
    "X_train = df_train[feature_names].to_numpy()\n",
    "y_train = df_train['PL'].to_numpy()\n",
    "X_test  = df_test[feature_names].to_numpy()\n",
    "y_test  = df_test['PL'].to_numpy()\n",
    "\n",
    "time_train = df_train['time'].to_numpy()\n",
    "time_test  = df_test['time'].to_numpy()\n",
    "\n",
    "ps = PredefinedSplit(fold_assignments)  # reuse the time-aware folds\n",
    "\n",
    "print(f\"Train: {len(df_train)} rows, Test: {len(df_test)} rows\")\n",
    "print(f\"Train window: {df_train.time.min()} -> {df_train.time.max()}\")\n",
    "print(f\"Test window:  {df_test.time.min()} -> {df_test.time.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a fraction of the data (keeps time order) *per fold*. Set USE_SUBSET=False for full data.\n",
    "USE_SUBSET = True\n",
    "DATA_FRACTION = 0.25  # fraction of data to keep from each fold\n",
    "\n",
    "if USE_SUBSET:\n",
    "    fold_ids = np.unique(fold_assignments[fold_assignments != -1])\n",
    "    keep_mask = np.zeros(len(df_train), dtype=bool)\n",
    "    kept_counts = {}\n",
    "\n",
    "    # keep the first DATA_FRACTION portion within each fold, preserving order\n",
    "    for fid in fold_ids:\n",
    "        idx = np.flatnonzero(fold_assignments == fid)\n",
    "        keep_n = max(1, int(np.ceil(len(idx) * DATA_FRACTION)))\n",
    "        keep_mask[idx[:keep_n]] = True\n",
    "        kept_counts[fid] = keep_n\n",
    "\n",
    "    # (optional) if there are -1 labels in train, subset them too\n",
    "    if np.any(fold_assignments == -1):\n",
    "        idx = np.flatnonzero(fold_assignments == -1)\n",
    "        keep_n = max(1, int(np.ceil(len(idx) * DATA_FRACTION)))\n",
    "        keep_mask[idx[:keep_n]] = True\n",
    "        kept_counts['-1'] = keep_n\n",
    "\n",
    "    df_train = df_train.iloc[keep_mask].copy()\n",
    "    fold_assignments = fold_assignments[keep_mask]\n",
    "    # subset test set separately if you still want a fraction\n",
    "    n_test = max(1, int(np.ceil(len(df_test) * DATA_FRACTION)))\n",
    "    df_test = df_test.iloc[:n_test].copy()\n",
    "\n",
    "    print(\"Using subset per fold:\", kept_counts)\n",
    "    print(f\"Train rows kept: {len(df_train)}, Test rows kept: {len(df_test)}\")\n",
    "else:\n",
    "    print(\"Using full dataset\")\n",
    "\n",
    "if len(df_train) == 0 or len(df_test) == 0:\n",
    "    raise ValueError(\"Subset produced empty data; increase DATA_FRACTION or disable USE_SUBSET.\")\n",
    "\n",
    "# refresh arrays/split objects\n",
    "X_train = df_train[feature_names].to_numpy()\n",
    "y_train = df_train[\"PL\"].to_numpy()\n",
    "X_test = df_test[feature_names].to_numpy()\n",
    "y_test = df_test[\"PL\"].to_numpy()\n",
    "time_train = df_train[\"time\"].to_numpy()\n",
    "time_test = df_test[\"time\"].to_numpy()\n",
    "ps = PredefinedSplit(fold_assignments)\n",
    "\n",
    "fold_ids = fold_assignments[fold_assignments != -1]\n",
    "print(\"Unique CV fold labels:\", np.unique(fold_ids), \"count:\", np.unique(fold_ids).size)\n",
    "print(\"Counts per fold:\", np.bincount(fold_ids) if fold_ids.size else \"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6c6322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (fit on train only)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Sequence length for RNN\n",
    "SEQ_LEN = 10\n",
    "\n",
    "def make_sequences(X, y, seq_len):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - seq_len + 1):\n",
    "        Xs.append(X[i:i + seq_len])\n",
    "        ys.append(y[i + seq_len - 1])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def build_sequences_by_fold(X, y, fold_assignments, seq_len):\n",
    "    X_list, y_list, fold_list = [], [], []\n",
    "    for fid in np.unique(fold_assignments):\n",
    "        idx = np.flatnonzero(fold_assignments == fid)\n",
    "        if idx.size < seq_len:\n",
    "            continue\n",
    "        X_fold = X[idx]\n",
    "        y_fold = y[idx]\n",
    "        X_seq, y_seq = make_sequences(X_fold, y_fold, seq_len)\n",
    "        X_list.append(X_seq)\n",
    "        y_list.append(y_seq)\n",
    "        fold_list.append(np.full(len(y_seq), fid, dtype=int))\n",
    "    if not X_list:\n",
    "        raise ValueError(\"No sequences created; reduce SEQ_LEN or check fold sizes.\")\n",
    "    return np.concatenate(X_list), np.concatenate(y_list), np.concatenate(fold_list)\n",
    "\n",
    "# Build sequences for training (respecting fold boundaries)\n",
    "X_train_seq, PL_train_seq, fold_seq = build_sequences_by_fold(\n",
    "    X_train_scaled, y_train, fold_assignments, SEQ_LEN\n",
    ")\n",
    "\n",
    "# Build sequences for test set\n",
    "if len(X_test_scaled) < SEQ_LEN:\n",
    "    raise ValueError(\"Test set shorter than SEQ_LEN; reduce SEQ_LEN or disable subset.\")\n",
    "X_test_seq, PL_test_seq = make_sequences(X_test_scaled, y_test, SEQ_LEN)\n",
    "\n",
    "# PredefinedSplit for sequence-level CV\n",
    "ps_seq = PredefinedSplit(fold_seq)\n",
    "\n",
    "print(f\"Train sequences: {X_train_seq.shape}, Test sequences: {X_test_seq.shape}\")\n",
    "print(\"Sequence fold labels:\", np.unique(fold_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bfac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(seq_len, n_features):\n",
    "    \"\"\"Creates a one-way RNN model for regression with regularization.\"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Explicit Input layer\n",
    "    model.add(Input(shape=(seq_len, n_features)))\n",
    "\n",
    "    # One-way RNN layer\n",
    "    model.add(SimpleRNN(32, activation='tanh', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Dense block\n",
    "    model.add(Dense(16, kernel_regularizer=l2(0.001)))\n",
    "    model.add(LeakyReLU(negative_slope=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Output layer for regression\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "seq_len = X_train_seq.shape[1]\n",
    "n_features = X_train_seq.shape[2]\n",
    "model = create_rnn_model(seq_len, n_features)\n",
    "model.summary()\n",
    "\n",
    "# Define Callbacks\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model with Callbacks\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    PL_train_seq,\n",
    "    validation_split=0.2,\n",
    "    epochs=500,\n",
    "    batch_size=128,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\nModel training completed...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a00bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on training and test data and display metrics in a table\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_loss, train_mae = model.evaluate(X_train_seq, PL_train_seq, verbose=0)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_mae = model.evaluate(X_test_seq, PL_test_seq, verbose=0)\n",
    "\n",
    "# Predict path loss for the test set\n",
    "PL_pred = model.predict(X_test_seq).flatten()\n",
    "\n",
    "# Calculate additional metrics\n",
    "rmse_test = np.sqrt(mean_squared_error(PL_test_seq, PL_pred))\n",
    "r2_test = r2_score(PL_test_seq, PL_pred)\n",
    "mape_test = mean_absolute_percentage_error(PL_test_seq, PL_pred)\n",
    "median_ae_test = median_absolute_error(PL_test_seq, PL_pred)\n",
    "\n",
    "# Create a results DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'Metric': ['Training Loss (MSE)', 'Training MAE',\n",
    "               'Test Loss (MSE)', 'Test MAE',\n",
    "               'Test RMSE', 'R2 Score',\n",
    "               'Test MAPE (%)', 'Test Median AE'],\n",
    "    'Value': [train_loss, train_mae,\n",
    "              test_loss, test_mae,\n",
    "              rmse_test, r2_test,\n",
    "              mape_test * 100, median_ae_test]\n",
    "})\n",
    "\n",
    "# Display the results table\n",
    "print(\"\\nModel Evaluation Metrics:\")\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history and model predictions\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss (MSE)')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss (MSE)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot MAE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot predicted vs actual values\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.scatterplot(x=PL_test_seq, y=PL_pred, alpha=0.6, edgecolor='w', s=70)\n",
    "plt.plot([PL_test_seq.min(), PL_test_seq.max()], [PL_test_seq.min(), PL_test_seq.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Path Loss')\n",
    "plt.ylabel('Predicted Path Loss')\n",
    "plt.title('Actual vs Predicted Path Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb3bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform time-aware cross-validation to assess model robustness and display results in a table\n",
    "\n",
    "seq_len = X_train_seq.shape[1]\n",
    "n_features = X_train_seq.shape[2]\n",
    "\n",
    "fold = 1\n",
    "cv_results_list = []\n",
    "\n",
    "for train_index, val_index in ps_seq.split():\n",
    "    print(f\"Training fold {fold}...\")\n",
    "\n",
    "    # Split data into training and validation for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_seq[train_index], X_train_seq[val_index]\n",
    "    PL_train_fold, PL_val_fold = PL_train_seq[train_index], PL_train_seq[val_index]\n",
    "\n",
    "    # Create a new instance of the model for each fold\n",
    "    model = create_rnn_model(seq_len, n_features)\n",
    "\n",
    "    # Define Callbacks\n",
    "    early_stop_cv = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=30,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    reduce_lr_cv = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-6,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Train the model on the current fold\n",
    "    history_cv = model.fit(\n",
    "        X_train_fold,\n",
    "        PL_train_fold,\n",
    "        validation_data=(X_val_fold, PL_val_fold),\n",
    "        epochs=500,\n",
    "        batch_size=128,\n",
    "        callbacks=[early_stop_cv, reduce_lr_cv],\n",
    "        verbose=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Predict path loss for the test set\n",
    "    PL_pred_cv = model.predict(X_test_seq).flatten()\n",
    "\n",
    "    # Calculate metrics\n",
    "    test_loss, test_mae = model.evaluate(X_test_seq, PL_test_seq, verbose=0)\n",
    "    rmse_cv = np.sqrt(mean_squared_error(PL_test_seq, PL_pred_cv))\n",
    "    r2_cv = r2_score(PL_test_seq, PL_pred_cv)\n",
    "    mape_cv = mean_absolute_percentage_error(PL_test_seq, PL_pred_cv)\n",
    "    median_ae_cv = median_absolute_error(PL_test_seq, PL_pred_cv)\n",
    "\n",
    "    # Append metrics to the results list\n",
    "    cv_results_list.append({\n",
    "        'Fold': fold,\n",
    "        'Test Loss (MSE)': round(test_loss, 4),\n",
    "        'Test MAE': round(test_mae, 4),\n",
    "        'Test RMSE': round(rmse_cv, 4),\n",
    "        'R2 Score': round(r2_cv, 4),\n",
    "        'Test MAPE (%)': round(mape_cv * 100, 2),\n",
    "        'Test Median AE': round(median_ae_cv, 4)\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"Fold {fold} - Test Loss (MSE): {test_loss:.4f}, Test MAE: {test_mae:.4f}, \"\n",
    "        f\"RMSE: {rmse_cv:.4f}, R2: {r2_cv:.4f}, MAPE: {mape_cv*100:.2f}%, \"\n",
    "        f\"Median AE: {median_ae_cv:.4f}\"\n",
    "    )\n",
    "    fold += 1\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "cv_results_df = pd.DataFrame(cv_results_list)\n",
    "\n",
    "# Calculate average and standard deviation for each metric\n",
    "cv_summary = cv_results_df.agg(['mean', 'std']).round(4).reset_index()\n",
    "cv_summary.rename(columns={'index': 'Metric'}, inplace=True)\n",
    "\n",
    "# Display Cross-Validation Results\n",
    "print(\"Time-Aware Cross-Validation Results:\")\n",
    "display(cv_results_df)\n",
    "\n",
    "print(\"Cross-Validation Summary:\")\n",
    "display(cv_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
