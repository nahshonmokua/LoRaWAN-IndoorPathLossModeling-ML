{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add8a05e-745f-42a8-b67f-8e486806dd9e",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Courier New', Courier, monospace; font-size: 30px; font-weight: bold; color: blue;  text-align: left;\">\n",
    "LightGBM for Regression \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f466d6-a3a9-456c-9e55-710ed03e0d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for data manipulation and visualization\n",
    "import numpy as np                               # For numerical operations\n",
    "import pandas as pd                              # For data manipulation\n",
    "import matplotlib.pyplot as plt                  # For plotting\n",
    "import seaborn as sns                            # For advanced data visualization\n",
    "\n",
    "# Libraries for model building and evaluation\n",
    "from sklearn.model_selection import (              # For cross-validation, splitting data, and grid search\n",
    "    KFold, \n",
    "    train_test_split, \n",
    "    GridSearchCV\n",
    ")\n",
    "from sklearn.metrics import (                     # For model evaluation metrics\n",
    "    mean_squared_error, \n",
    "    r2_score, \n",
    "    mean_absolute_percentage_error, \n",
    "    median_absolute_error\n",
    ")\n",
    "\n",
    "# LightGBM library\n",
    "import lightgbm as lgb                            # For LightGBM Regressor\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44557a3-9ab1-490c-82d0-c7f1cd62bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "dataset_path = '../../all_data_files/cleaned_dataset_per_device.csv'\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at the specified path: {dataset_path}\")\n",
    "    import sys\n",
    "    sys.exit()\n",
    "\n",
    "# Display dataset information\n",
    "print(\"\\nDataset Information:\")\n",
    "df.info()\n",
    "print(\"\\nFirst Five Rows of the Dataset:\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06491f8-bf9a-41d6-9474-6e558d9d6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns and target\n",
    "feature_columns = [\n",
    "    'distance', 'frequency', 'c_walls', 'w_walls', \n",
    "    'co2', 'humidity', 'pm25', 'pressure', \n",
    "    'temperature', 'snr'\n",
    "]\n",
    "target_column = 'exp_pl'\n",
    "\n",
    "# Verify that all required columns exist\n",
    "missing_columns = set(feature_columns + [target_column]) - set(df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"The following required columns are missing in the dataset: {missing_columns}\")\n",
    "\n",
    "# Extract features and target\n",
    "all_features = df[feature_columns].values\n",
    "PL_all = df[target_column].values\n",
    "\n",
    "# Perform train-test split (80-20 split)\n",
    "X_train_all, X_test_all, PL_train_all, PL_test_all = train_test_split(\n",
    "    all_features, PL_all, test_size=0.2, random_state=50\n",
    ")\n",
    "\n",
    "print(\"Train-test split completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa9051-9b65-45eb-8e86-a7728d449eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fixed max_depth values\n",
    "max_depth_values = [1, 2, 3]\n",
    "\n",
    "# Define other hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50, 100],                  # Number of leaves in one tree\n",
    "    'learning_rate': [0.01, 0.1, 0.2],            # Step size shrinkage\n",
    "    'n_estimators': [100, 200, 300],              # Number of boosting iterations\n",
    "    'subsample': [0.6, 0.8, 1.0],                 # Subsample ratio of the training instances\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],          # Subsample ratio of columns when constructing each tree\n",
    "    'min_child_weight': [1, 2, 4],                # Minimum sum of instance weight (hessian) needed in a child\n",
    "    'reg_alpha': [0, 0.1, 0.5],                   # L1 regularization term on weights\n",
    "    'reg_lambda': [0, 0.1, 0.5]                   # L2 regularization term on weights\n",
    "}\n",
    "\n",
    "# Dictionary to store the best models for each max_depth\n",
    "best_models = {}\n",
    "best_params_per_depth = {}\n",
    "best_scores_per_depth = {}\n",
    "\n",
    "# Iterate over each max_depth and perform grid search\n",
    "for depth in max_depth_values:\n",
    "    print(f\"\\nPerforming Grid Search for max_depth={depth}...\")\n",
    "    \n",
    "    # Create a base LightGBM Regressor with the current max_depth\n",
    "    lgb_reg = lgb.LGBMRegressor(\n",
    "        max_depth=depth,\n",
    "        objective='regression',                       # Regression task\n",
    "        random_state=50, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Initialize Grid Search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=lgb_reg,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,                                         # 5-fold cross-validation\n",
    "        scoring='neg_mean_squared_error',             # Using negative MSE for comparison\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Perform Grid Search\n",
    "    grid_search.fit(X_train_all, PL_train_all)\n",
    "    \n",
    "    # Retrieve the best parameters and corresponding score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_neg_mse = grid_search.best_score_\n",
    "    best_mse = -best_neg_mse  # Convert from negative MSE to MSE\n",
    "    \n",
    "    best_models[depth] = grid_search.best_estimator_\n",
    "    best_params_per_depth[depth] = best_params\n",
    "    best_scores_per_depth[depth] = best_mse\n",
    "    \n",
    "    print(f\"Best Parameters for max_depth={depth}: {best_params}\")\n",
    "    print(f\"Best CV MSE for max_depth={depth}: {best_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2a249-b743-4e38-84d2-732857079fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store evaluation metrics for each model\n",
    "evaluation_metrics = []\n",
    "\n",
    "for depth in max_depth_values:\n",
    "    model = best_models[depth]\n",
    "    params = best_params_per_depth[depth]\n",
    "    \n",
    "    print(f\"\\nEvaluating model with max_depth={depth} and parameters: {params}\")\n",
    "    \n",
    "    # Train the model on the entire training set\n",
    "    model.fit(X_train_all, PL_train_all)\n",
    "    \n",
    "    # Retrieve OOB score if applicable (LightGBM does not have OOB, but can use other metrics)\n",
    "    # Instead, we'll use the training data evaluation\n",
    "    # Make predictions\n",
    "    PL_train_pred = model.predict(X_train_all)\n",
    "    PL_test_pred = model.predict(X_test_all)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mse = mean_squared_error(PL_train_all, PL_train_pred)\n",
    "    test_mse = mean_squared_error(PL_test_all, PL_test_pred)\n",
    "    train_r2 = r2_score(PL_train_all, PL_train_pred)\n",
    "    test_r2 = r2_score(PL_test_all, PL_test_pred)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_mape = mean_absolute_percentage_error(PL_test_all, PL_test_pred)\n",
    "    test_median_ae = median_absolute_error(PL_test_all, PL_test_pred)\n",
    "    \n",
    "    # Append metrics to the list\n",
    "    evaluation_metrics.append({\n",
    "        'max_depth': depth,\n",
    "        'Training Loss (MSE)': train_mse,\n",
    "        'Test Loss (MSE)': test_mse,\n",
    "        'Test RMSE': test_rmse,\n",
    "        'R² Score': test_r2,\n",
    "        'Test MAPE (%)': test_mape * 100,\n",
    "        'Test Median AE': test_median_ae\n",
    "    })\n",
    "    \n",
    "    print(f\"Model with max_depth={depth} - Training MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}\")\n",
    "    print(f\"Training R²: {train_r2:.4f}, Test R²: {test_r2:.4f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.4f}, Test MAPE: {test_mape*100:.2f}%, Test Median AE: {test_median_ae:.4f}\")\n",
    "    \n",
    "# Create a results DataFrame\n",
    "results_df = pd.DataFrame(evaluation_metrics)\n",
    "\n",
    "print(\"\\nModel Evaluation Metrics for Best Models per max_depth:\")\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723fe50e-4e12-4ec6-95cd-66378827f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of models\n",
    "num_models = len(max_depth_values)\n",
    "\n",
    "# Set up the subplot grid (1 row, 3 columns)\n",
    "fig, axes = plt.subplots(1, num_models, figsize=(18, 5), sharey=True)\n",
    "\n",
    "# If only one subplot, make axes iterable\n",
    "if num_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, depth in zip(axes, max_depth_values):\n",
    "    model = best_models[depth]\n",
    "    PL_test_pred = model.predict(X_test_all)\n",
    "    \n",
    "    sns.scatterplot(x=PL_test_all, y=PL_test_pred, alpha=0.5, edgecolor='w', s=50, ax=ax)\n",
    "    ax.plot([PL_test_all.min(), PL_test_all.max()], [PL_test_all.min(), PL_test_all.max()], 'r--', lw=2)\n",
    "    ax.set_xlabel('Actual Path Loss')\n",
    "    ax.set_ylabel('Predicted Path Loss')\n",
    "    ax.set_title(f'Actual vs Predicted Path Loss (max_depth={depth})')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b49ae8e-29a5-4b62-a1e4-306362c2ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds for cross-validation\n",
    "n_folds = 5\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Dictionary to store cross-validation results\n",
    "cv_results_dict = {depth: [] for depth in max_depth_values}\n",
    "\n",
    "print(\"\\nPerforming K-Fold Cross-Validation for Each Best Model...\\n\")\n",
    "\n",
    "for depth in max_depth_values:\n",
    "    model = best_models[depth]\n",
    "    params = best_params_per_depth[depth]\n",
    "    \n",
    "    print(f\"Cross-Validation for max_depth={depth} with parameters: {params}\")\n",
    "    \n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kf.split(X_train_all):\n",
    "        print(f\"  Training fold {fold}...\")\n",
    "        \n",
    "        # Split the data for the current fold\n",
    "        X_train_fold, X_val_fold = X_train_all[train_idx], X_train_all[val_idx]\n",
    "        PL_train_fold, PL_val_fold = PL_train_all[train_idx], PL_train_all[val_idx]\n",
    "        \n",
    "        # Instantiate a new model with the best parameters\n",
    "        lgb_cv = lgb.LGBMRegressor(\n",
    "            max_depth=depth,\n",
    "            num_leaves=params['num_leaves'],\n",
    "            learning_rate=params['learning_rate'],\n",
    "            n_estimators=params['n_estimators'],\n",
    "            subsample=params['subsample'],\n",
    "            colsample_bytree=params['colsample_bytree'],\n",
    "            min_child_weight=params['min_child_weight'],\n",
    "            reg_alpha=params['reg_alpha'],\n",
    "            reg_lambda=params['reg_lambda'],\n",
    "            objective='regression',\n",
    "            random_state=50, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Train the model on the current fold\n",
    "        lgb_cv.fit(X_train_fold, PL_train_fold)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        PL_val_pred = lgb_cv.predict(X_val_fold)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        val_mse = mean_squared_error(PL_val_fold, PL_val_pred)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        val_r2 = r2_score(PL_val_fold, PL_val_pred)\n",
    "        val_mape = mean_absolute_percentage_error(PL_val_fold, PL_val_pred)\n",
    "        val_median_ae = median_absolute_error(PL_val_fold, PL_val_pred)\n",
    "        \n",
    "        # Append metrics to the dictionary\n",
    "        cv_results_dict[depth].append({\n",
    "            'Fold': fold,\n",
    "            'Validation Loss (MSE)': round(val_mse, 4),\n",
    "            'Validation RMSE': round(val_rmse, 4),\n",
    "            'R² Score': round(val_r2, 4),\n",
    "            'Validation MAPE (%)': round(val_mape * 100, 2),\n",
    "            'Validation Median AE': round(val_median_ae, 4)\n",
    "        })\n",
    "        \n",
    "        print(f\"    Fold {fold} - MSE: {val_mse:.4f}, RMSE: {val_rmse:.4f}, R²: {val_r2:.4f}, MAPE: {val_mape*100:.2f}%, Median AE: {val_median_ae:.4f}\\n\")\n",
    "        fold += 1\n",
    "\n",
    "# Create a DataFrame for each max_depth's CV results and display them\n",
    "for depth in max_depth_values:\n",
    "    print(f\"\\nK-Fold Cross-Validation Results for max_depth={depth}:\")\n",
    "    cv_results_df = pd.DataFrame(cv_results_dict[depth])\n",
    "    display(cv_results_df)\n",
    "    \n",
    "    # Summary statistics\n",
    "    cv_summary = cv_results_df.agg(['mean', 'std']).round(4).reset_index()\n",
    "    cv_summary.rename(columns={'index': 'Metric'}, inplace=True)\n",
    "    print(f\"\\nCross-Validation Summary for max_depth={depth}:\")\n",
    "    display(cv_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea6033-2d80-498b-92c5-70aec19a93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the number of models and metrics\n",
    "num_models = len(max_depth_values)\n",
    "metrics_to_plot = ['Validation RMSE', 'R² Score', 'Validation MAPE (%)']\n",
    "\n",
    "# Set up the subplot grid (rows: number of metrics, columns: number of models)\n",
    "fig, axes = plt.subplots(len(metrics_to_plot), num_models, figsize=(6 * num_models, 5 * len(metrics_to_plot)))\n",
    "\n",
    "# Flatten axes for easy iteration if necessary\n",
    "if num_models == 1:\n",
    "    axes = np.array(axes).reshape(len(metrics_to_plot), 1)\n",
    "\n",
    "for j, metric in enumerate(metrics_to_plot):\n",
    "    for i, depth in enumerate(max_depth_values):\n",
    "        ax = axes[j, i]\n",
    "        cv_results_df = pd.DataFrame(cv_results_dict[depth])\n",
    "        sns.barplot(x='Fold', y=metric, data=cv_results_df, palette='viridis', ax=ax)\n",
    "        ax.set_title(f'max_depth={depth} - {metric}')\n",
    "        ax.set_xlabel('Fold')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.grid(True, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
